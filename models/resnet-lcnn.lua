local nn = require 'nn'
require 'cunn'

local ConvolutionFull = cudnn.SpatialConvolution
local ConvolutionPool = cudnn.PooledSpatialConvolution
local Avg = cudnn.SpatialAveragePooling
local ReLU = cudnn.ReLU
local Max = nn.SpatialMaxPooling
local SBatchNorm = nn.SpatialBatchNormalization

function LRregime(epoch, optimType)
   local regimes = {
      -- start, end,    LR,   WD,
      {  1,      7,   1e-1,   5e-4, },
      {  8,     16,   3e-2,   5e-4  },
      { 17,     25,   1e-2,   0 },
      { 26,     30,   3e-3,   0 },
      { 31,    1e8,   1e-3,   0 },
   }
   local params, newRegime
   for _, row in ipairs(regimes) do
      if epoch >= row[1] and epoch <= row[2] then
         params, newRegime = { learningRate=row[3], weightDecay=row[4] }, epoch == row[1]
         break
      end
   end
   params.learningRate = params.learningRate * opt.LR
   return params, newRegime
end


function createModel()
   local depth = opt.depth
   local shortcutType = opt.shortcutType or 'B'
   local iChannels

   local function Convolution(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH, poolsize)
      if poolsize and poolsize >= nOutputPlane*kW*kH then -- number of basis >= number of vectors to be coded
        poolsize = nil
      end
      local ConvLayer = poolsize and ConvolutionPool or ConvolutionFull
      return ConvLayer(nInputPlane, nOutputPlane, kW, kH, dW, dH, padW, padH, poolsize)
   end

   -- The shortcut layer is either identity or 1x1 convolution
   local function shortcut(nInputPlane, nOutputPlane, stride)
      local useConv = shortcutType == 'C' or
         (shortcutType == 'B' and nInputPlane ~= nOutputPlane)
      if useConv then
         -- 1x1 convolution
         return nn.Sequential()
            :add(Convolution(nInputPlane, nOutputPlane, 1, 1, stride, stride, nil, nil, poolsize))
            :add(SBatchNorm(nOutputPlane))
      elseif nInputPlane ~= nOutputPlane then
         -- Strided, zero-padded identity shortcut
         return nn.Sequential()
            :add(nn.SpatialAveragePooling(1, 1, stride, stride))
            :add(nn.Concat(2)
               :add(nn.Identity())
               :add(nn.MulConstant(0)))
      else
         return nn.Identity()
      end
   end

   -- The basic residual layer block for 18 and 34 layer network, and the
   -- CIFAR networks
   local function basicblock(n, stride, poolsize)
      local nInputPlane = iChannels
      iChannels = n

      local s = nn.Sequential()
      s:add(Convolution(nInputPlane,n,3,3,stride,stride,1,1, poolsize))
      s:add(SBatchNorm(n))
      s:add(ReLU(true))
      s:add(Convolution(n,n,3,3,1,1,1,1, poolsize))
      s:add(SBatchNorm(n))

      return nn.Sequential()
         :add(nn.ConcatTable()
            :add(s)
            :add(shortcut(nInputPlane, n, stride)))
         :add(nn.CAddTable(true))
         :add(ReLU(true))
   end

   -- The bottleneck residual layer for 50, 101, and 152 layer networks
   local function bottleneck(n, stride, poolsize)
      local nInputPlane = iChannels
      iChannels = n * 4

      local s = nn.Sequential()
      s:add(Convolution(nInputPlane,n,1,1,1,1,0,0, poolsize)) -- nInputPlane * n
      s:add(SBatchNorm(n))
      s:add(ReLU(true))
      s:add(Convolution(n,n,3,3,stride,stride,1,1, poolsize)) -- n * n * 3 * 3
      s:add(SBatchNorm(n))
      s:add(ReLU(true))
      s:add(Convolution(n,n*4,1,1,1,1,0,0, poolsize)) -- n * n * 4 or 0 -- Depends on shortcut type.
      s:add(SBatchNorm(n * 4))

      return nn.Sequential()
         :add(nn.ConcatTable()
            :add(s)
            :add(shortcut(nInputPlane, n * 4, stride))) -- nInputPlane * n * 4
         :add(nn.CAddTable(true))
         :add(ReLU(true))
   end

   -- Creates count residual blocks with specified number of features
   local function layer(block, features, count, stride, poolsize)
      local s = nn.Sequential()
      for i=1,count do
         s:add(block(features, i == 1 and stride or 1, poolsize))
      end
      return s
   end

   local model = nn.Sequential()
   if opt.dataset == 'imagenet' then
      -- Configurations for ResNet:
      --  num. residual blocks, num features, residual block function
      local cfg = {
         [8]  = {{1, 1, 1, 1}, 512, basicblock},
         [18]  = {{2, 2, 2, 2}, 512, basicblock},
         [34]  = {{3, 4, 6, 3}, 512, basicblock},
         [50]  = {{3, 4, 6, 3}, 2048, bottleneck},
         [101] = {{3, 4, 23, 3}, 2048, bottleneck},
         [152] = {{3, 8, 36, 3}, 2048, bottleneck},
      }

      assert(cfg[depth], 'Invalid depth: ' .. tostring(depth))
      local def, nFeatures, block = table.unpack(cfg[depth])
      iChannels = 64
      print(' | ResNet-' .. depth .. ' ImageNet')

      -- The ResNet ImageNet model
      model:add(Convolution(3,64,7,7,2,2,3,3, opt.firstPool))
      model:add(SBatchNorm(64))
      model:add(ReLU(true))
      model:add(Max(3,3,2,2,1,1))

      model:add(layer(block, 64, def[1], 1, opt.pool1))
      model:add(layer(block, 128, def[2], 2, opt.pool2))
      model:add(layer(block, 256, def[3], 2, opt.pool3))
      model:add(layer(block, 512, def[4], 2, opt.pool4))
      model:add(Avg(7, 7, 1, 1))
      model:add(nn.View(nFeatures):setNumInputDims(3))
      model:add(nn.Linear(nFeatures, 1000))
      model:add(nn.LogSoftMax())
   else
      error('invalid dataset: ' .. opt.dataset)
   end

   model.LRregime = LRregime
   return model
end
